25/11/25 00:14:26 INFO SparkContext: Running Spark version 3.5.7
25/11/25 00:14:26 INFO SparkContext: OS info Linux, 6.8.0-64-generic, amd64
25/11/25 00:14:26 INFO SparkContext: Java version 1.8.0_472
25/11/25 00:14:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/11/25 00:14:26 INFO ResourceUtils: ==============================================================
25/11/25 00:14:26 INFO ResourceUtils: No custom resources configured for spark.driver.
25/11/25 00:14:26 INFO ResourceUtils: ==============================================================
25/11/25 00:14:26 INFO SparkContext: Submitted application: SparkVault
25/11/25 00:14:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/11/25 00:14:26 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
25/11/25 00:14:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/11/25 00:14:26 INFO SecurityManager: Changing view acls to: hadoop
25/11/25 00:14:26 INFO SecurityManager: Changing modify acls to: hadoop
25/11/25 00:14:26 INFO SecurityManager: Changing view acls groups to: 
25/11/25 00:14:26 INFO SecurityManager: Changing modify acls groups to: 
25/11/25 00:14:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
25/11/25 00:14:26 INFO Utils: Successfully started service 'sparkDriver' on port 40457.
25/11/25 00:14:26 INFO SparkEnv: Registering MapOutputTracker
25/11/25 00:14:26 INFO SparkEnv: Registering BlockManagerMaster
25/11/25 00:14:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/11/25 00:14:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/11/25 00:14:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/11/25 00:14:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd4c1e0d-d884-47a2-b458-43e7d6247bae
25/11/25 00:14:26 INFO MemoryStore: MemoryStore started with capacity 2004.6 MiB
25/11/25 00:14:26 INFO SparkEnv: Registering OutputCommitCoordinator
25/11/25 00:14:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/11/25 00:14:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/11/25 00:14:27 INFO SparkContext: Added JAR file:/home/hadoop/chamaleon-hadoop/spark/target/spark-vault-1.0-SNAPSHOT.jar at spark://hadoop-master:40457/jars/spark-vault-1.0-SNAPSHOT.jar with timestamp 1764029666287
25/11/25 00:14:27 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032
25/11/25 00:14:27 INFO Configuration: resource-types.xml not found
25/11/25 00:14:27 INFO ResourceUtils: Unable to find 'resource-types.xml'.
25/11/25 00:14:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)
25/11/25 00:14:27 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
25/11/25 00:14:27 INFO Client: Setting up container launch context for our AM
25/11/25 00:14:27 INFO Client: Setting up the launch environment for our AM container
25/11/25 00:14:27 INFO Client: Preparing resources for our AM container
25/11/25 00:14:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
25/11/25 00:14:29 INFO Client: Uploading resource file:/tmp/spark-549a2de3-53e0-4be5-a1f9-a7129694336d/__spark_libs__2786156140626296655.zip -> hdfs://hadoop-master:9000/user/hadoop/.sparkStaging/application_1764027644285_0004/__spark_libs__2786156140626296655.zip
25/11/25 00:14:31 INFO Client: Uploading resource file:/tmp/spark-549a2de3-53e0-4be5-a1f9-a7129694336d/__spark_conf__1403730600783865731.zip -> hdfs://hadoop-master:9000/user/hadoop/.sparkStaging/application_1764027644285_0004/__spark_conf__.zip
25/11/25 00:14:31 INFO SecurityManager: Changing view acls to: hadoop
25/11/25 00:14:31 INFO SecurityManager: Changing modify acls to: hadoop
25/11/25 00:14:31 INFO SecurityManager: Changing view acls groups to: 
25/11/25 00:14:31 INFO SecurityManager: Changing modify acls groups to: 
25/11/25 00:14:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
25/11/25 00:14:31 INFO Client: Submitting application application_1764027644285_0004 to ResourceManager
25/11/25 00:14:31 INFO YarnClientImpl: Submitted application application_1764027644285_0004
25/11/25 00:14:32 INFO Client: Application report for application_1764027644285_0004 (state: ACCEPTED)
25/11/25 00:14:32 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1764029671320
	 final status: UNDEFINED
	 tracking URL: http://gecuvm:8088/proxy/application_1764027644285_0004/
	 user: hadoop
25/11/25 00:14:36 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> gecuvm, PROXY_URI_BASES -> http://gecuvm:8088/proxy/application_1764027644285_0004), /proxy/application_1764027644285_0004
25/11/25 00:14:36 INFO Client: Application report for application_1764027644285_0004 (state: RUNNING)
25/11/25 00:14:36 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.52.3.12
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1764029671320
	 final status: UNDEFINED
	 tracking URL: http://gecuvm:8088/proxy/application_1764027644285_0004/
	 user: hadoop
25/11/25 00:14:36 INFO YarnClientSchedulerBackend: Application application_1764027644285_0004 has started running.
25/11/25 00:14:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35653.
25/11/25 00:14:36 INFO NettyBlockTransferService: Server created on hadoop-master:35653
25/11/25 00:14:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/11/25 00:14:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop-master, 35653, None)
25/11/25 00:14:36 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-master:35653 with 2004.6 MiB RAM, BlockManagerId(driver, hadoop-master, 35653, None)
25/11/25 00:14:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop-master, 35653, None)
25/11/25 00:14:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop-master, 35653, None)
25/11/25 00:14:36 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:36 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:14:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.52.3.12:43528) with ID 1,  ResourceProfileId 0
25/11/25 00:14:39 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
Generating 1073741824 records to hdfs:///hw5/data-16GB
25/11/25 00:14:40 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-master:45319 with 912.3 MiB RAM, BlockManagerId(1, hadoop-master, 45319, None)
25/11/25 00:14:40 INFO SparkContext: Starting job: sortByKey at SparkVault.java:111
25/11/25 00:14:40 INFO DAGScheduler: Got job 0 (sortByKey at SparkVault.java:111) with 64 output partitions
25/11/25 00:14:40 INFO DAGScheduler: Final stage: ResultStage 0 (sortByKey at SparkVault.java:111)
25/11/25 00:14:40 INFO DAGScheduler: Parents of final stage: List()
25/11/25 00:14:40 INFO DAGScheduler: Missing parents: List()
25/11/25 00:14:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at sortByKey at SparkVault.java:111), which has no missing parents
25/11/25 00:14:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.4 KiB, free 2004.6 MiB)
25/11/25 00:14:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KiB, free 2004.6 MiB)
25/11/25 00:14:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-master:35653 (size: 3.3 KiB, free: 2004.6 MiB)
25/11/25 00:14:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1611
25/11/25 00:14:40 INFO DAGScheduler: Submitting 64 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at sortByKey at SparkVault.java:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
25/11/25 00:14:40 INFO YarnScheduler: Adding task set 0.0 with 64 tasks resource profile 0
25/11/25 00:14:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hadoop-master, executor 1, partition 0, PROCESS_LOCAL, 9199 bytes) 
25/11/25 00:14:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-master:45319 (size: 3.3 KiB, free: 912.3 MiB)
25/11/25 00:15:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (hadoop-master, executor 1, partition 1, PROCESS_LOCAL, 9204 bytes) 
25/11/25 00:15:03 ERROR TaskSetManager: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,16777216,[Ljava.lang.Object;@47e769bd))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1); not retrying
25/11/25 00:15:03 INFO YarnScheduler: Cancelling stage 0
25/11/25 00:15:03 INFO YarnScheduler: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,16777216,[Ljava.lang.Object;@47e769bd))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1)
25/11/25 00:15:04 INFO YarnScheduler: Stage 0 was cancelled
25/11/25 00:15:04 INFO DAGScheduler: ResultStage 0 (sortByKey at SparkVault.java:111) failed in 23.607 s due to Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,16777216,[Ljava.lang.Object;@47e769bd))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1)
25/11/25 00:15:04 INFO DAGScheduler: Job 0 failed: sortByKey at SparkVault.java:111, took 23.675523 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,16777216,[Ljava.lang.Object;@47e769bd))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)
	at org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)
	at org.apache.spark.api.java.JavaPairRDD.sortByKey(JavaPairRDD.scala:927)
	at org.apache.spark.api.java.JavaPairRDD.sortByKey(JavaPairRDD.scala:917)
	at edu.iit.cs553.SparkVault.runGeneration(SparkVault.java:111)
	at edu.iit.cs553.SparkVault.main(SparkVault.java:52)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1034)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
25/11/25 00:15:04 INFO SparkContext: Invoking stop() from shutdown hook
25/11/25 00:15:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/11/25 00:15:04 INFO SparkUI: Stopped Spark web UI at http://hadoop-master:4040
25/11/25 00:15:04 INFO YarnClientSchedulerBackend: Interrupting monitor thread
25/11/25 00:15:04 INFO YarnClientSchedulerBackend: Shutting down all executors
25/11/25 00:15:04 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/11/25 00:15:04 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
25/11/25 00:15:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/11/25 00:15:04 INFO MemoryStore: MemoryStore cleared
25/11/25 00:15:04 INFO BlockManager: BlockManager stopped
25/11/25 00:15:04 INFO BlockManagerMaster: BlockManagerMaster stopped
25/11/25 00:15:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/11/25 00:15:04 INFO SparkContext: Successfully stopped SparkContext
25/11/25 00:15:04 INFO ShutdownHookManager: Shutdown hook called
25/11/25 00:15:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-549a2de3-53e0-4be5-a1f9-a7129694336d
25/11/25 00:15:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-0b39841f-d04e-404f-8a70-55ed03fb978d

real	0m39.939s
user	0m25.232s
sys	0m3.513s
