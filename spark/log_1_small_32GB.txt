25/11/25 00:15:07 INFO SparkContext: Running Spark version 3.5.7
25/11/25 00:15:07 INFO SparkContext: OS info Linux, 6.8.0-64-generic, amd64
25/11/25 00:15:07 INFO SparkContext: Java version 1.8.0_472
25/11/25 00:15:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/11/25 00:15:08 INFO ResourceUtils: ==============================================================
25/11/25 00:15:08 INFO ResourceUtils: No custom resources configured for spark.driver.
25/11/25 00:15:08 INFO ResourceUtils: ==============================================================
25/11/25 00:15:08 INFO SparkContext: Submitted application: SparkVault
25/11/25 00:15:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/11/25 00:15:08 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
25/11/25 00:15:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/11/25 00:15:08 INFO SecurityManager: Changing view acls to: hadoop
25/11/25 00:15:08 INFO SecurityManager: Changing modify acls to: hadoop
25/11/25 00:15:08 INFO SecurityManager: Changing view acls groups to: 
25/11/25 00:15:08 INFO SecurityManager: Changing modify acls groups to: 
25/11/25 00:15:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
25/11/25 00:15:08 INFO Utils: Successfully started service 'sparkDriver' on port 34523.
25/11/25 00:15:08 INFO SparkEnv: Registering MapOutputTracker
25/11/25 00:15:08 INFO SparkEnv: Registering BlockManagerMaster
25/11/25 00:15:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/11/25 00:15:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/11/25 00:15:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/11/25 00:15:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6b2cc525-528b-4be3-a2fb-f50ffdd205b7
25/11/25 00:15:08 INFO MemoryStore: MemoryStore started with capacity 2004.6 MiB
25/11/25 00:15:08 INFO SparkEnv: Registering OutputCommitCoordinator
25/11/25 00:15:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/11/25 00:15:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/11/25 00:15:08 INFO SparkContext: Added JAR file:/home/hadoop/chamaleon-hadoop/spark/target/spark-vault-1.0-SNAPSHOT.jar at spark://hadoop-master:34523/jars/spark-vault-1.0-SNAPSHOT.jar with timestamp 1764029707949
25/11/25 00:15:09 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032
25/11/25 00:15:09 INFO Configuration: resource-types.xml not found
25/11/25 00:15:09 INFO ResourceUtils: Unable to find 'resource-types.xml'.
25/11/25 00:15:09 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)
25/11/25 00:15:09 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
25/11/25 00:15:09 INFO Client: Setting up container launch context for our AM
25/11/25 00:15:09 INFO Client: Setting up the launch environment for our AM container
25/11/25 00:15:09 INFO Client: Preparing resources for our AM container
25/11/25 00:15:09 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
25/11/25 00:15:11 INFO Client: Uploading resource file:/tmp/spark-fea6283c-95ce-48c2-a83a-4c31f0e854bb/__spark_libs__7063135177349589788.zip -> hdfs://hadoop-master:9000/user/hadoop/.sparkStaging/application_1764027644285_0005/__spark_libs__7063135177349589788.zip
25/11/25 00:15:13 INFO Client: Uploading resource file:/tmp/spark-fea6283c-95ce-48c2-a83a-4c31f0e854bb/__spark_conf__1575846727484358402.zip -> hdfs://hadoop-master:9000/user/hadoop/.sparkStaging/application_1764027644285_0005/__spark_conf__.zip
25/11/25 00:15:13 INFO SecurityManager: Changing view acls to: hadoop
25/11/25 00:15:13 INFO SecurityManager: Changing modify acls to: hadoop
25/11/25 00:15:13 INFO SecurityManager: Changing view acls groups to: 
25/11/25 00:15:13 INFO SecurityManager: Changing modify acls groups to: 
25/11/25 00:15:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hadoop; groups with view permissions: EMPTY; users with modify permissions: hadoop; groups with modify permissions: EMPTY
25/11/25 00:15:13 INFO Client: Submitting application application_1764027644285_0005 to ResourceManager
25/11/25 00:15:13 INFO YarnClientImpl: Submitted application application_1764027644285_0005
25/11/25 00:15:14 INFO Client: Application report for application_1764027644285_0005 (state: ACCEPTED)
25/11/25 00:15:14 INFO Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1764029713222
	 final status: UNDEFINED
	 tracking URL: http://gecuvm:8088/proxy/application_1764027644285_0005/
	 user: hadoop
25/11/25 00:15:18 INFO Client: Application report for application_1764027644285_0005 (state: RUNNING)
25/11/25 00:15:18 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.52.3.12
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1764029713222
	 final status: UNDEFINED
	 tracking URL: http://gecuvm:8088/proxy/application_1764027644285_0005/
	 user: hadoop
25/11/25 00:15:18 INFO YarnClientSchedulerBackend: Application application_1764027644285_0005 has started running.
25/11/25 00:15:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46199.
25/11/25 00:15:18 INFO NettyBlockTransferService: Server created on hadoop-master:46199
25/11/25 00:15:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/11/25 00:15:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop-master, 46199, None)
25/11/25 00:15:18 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-master:46199 with 2004.6 MiB RAM, BlockManagerId(driver, hadoop-master, 46199, None)
25/11/25 00:15:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop-master, 46199, None)
25/11/25 00:15:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop-master, 46199, None)
25/11/25 00:15:18 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> gecuvm, PROXY_URI_BASES -> http://gecuvm:8088/proxy/application_1764027644285_0005), /proxy/application_1764027644285_0005
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
25/11/25 00:15:18 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
25/11/25 00:15:22 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.52.3.12:39168) with ID 1,  ResourceProfileId 0
25/11/25 00:15:22 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
Generating 2147483648 records to hdfs:///hw5/data-32GB
25/11/25 00:15:22 INFO BlockManagerMasterEndpoint: Registering block manager hadoop-master:44585 with 912.3 MiB RAM, BlockManagerId(1, hadoop-master, 44585, None)
25/11/25 00:15:23 INFO SparkContext: Starting job: sortByKey at SparkVault.java:111
25/11/25 00:15:23 INFO DAGScheduler: Got job 0 (sortByKey at SparkVault.java:111) with 64 output partitions
25/11/25 00:15:23 INFO DAGScheduler: Final stage: ResultStage 0 (sortByKey at SparkVault.java:111)
25/11/25 00:15:23 INFO DAGScheduler: Parents of final stage: List()
25/11/25 00:15:23 INFO DAGScheduler: Missing parents: List()
25/11/25 00:15:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at sortByKey at SparkVault.java:111), which has no missing parents
25/11/25 00:15:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.4 KiB, free 2004.6 MiB)
25/11/25 00:15:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.3 KiB, free 2004.6 MiB)
25/11/25 00:15:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-master:46199 (size: 3.3 KiB, free: 2004.6 MiB)
25/11/25 00:15:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1611
25/11/25 00:15:23 INFO DAGScheduler: Submitting 64 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at sortByKey at SparkVault.java:111) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
25/11/25 00:15:23 INFO YarnScheduler: Adding task set 0.0 with 64 tasks resource profile 0
25/11/25 00:15:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hadoop-master, executor 1, partition 0, PROCESS_LOCAL, 9199 bytes) 
25/11/25 00:15:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop-master:44585 (size: 3.3 KiB, free: 912.3 MiB)
25/11/25 00:16:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (hadoop-master, executor 1, partition 1, PROCESS_LOCAL, 9204 bytes) 
25/11/25 00:16:34 ERROR TaskSetManager: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,33554432,[Ljava.lang.Object;@6b5fef53))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1); not retrying
25/11/25 00:16:34 INFO YarnScheduler: Cancelling stage 0
25/11/25 00:16:34 INFO YarnScheduler: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,33554432,[Ljava.lang.Object;@6b5fef53))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1)
25/11/25 00:16:34 INFO YarnScheduler: Stage 0 was cancelled
25/11/25 00:16:34 INFO DAGScheduler: ResultStage 0 (sortByKey at SparkVault.java:111) failed in 71.437 s due to Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,33554432,[Ljava.lang.Object;@6b5fef53))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1)
25/11/25 00:16:34 INFO DAGScheduler: Job 0 failed: sortByKey at SparkVault.java:111, took 71.500114 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: java.nio.HeapByteBuffer
Serialization stack:
	- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=10 cap=10])
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 60)
	- field (class: scala.Tuple3, name: _3, type: class java.lang.Object)
	- object (class scala.Tuple3, (0,33554432,[Ljava.lang.Object;@6b5fef53))
	- element of array (index: 0)
	- array (class [Lscala.Tuple3;, size 1)
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)
	at org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)
	at org.apache.spark.api.java.JavaPairRDD.sortByKey(JavaPairRDD.scala:927)
	at org.apache.spark.api.java.JavaPairRDD.sortByKey(JavaPairRDD.scala:917)
	at edu.iit.cs553.SparkVault.runGeneration(SparkVault.java:111)
	at edu.iit.cs553.SparkVault.main(SparkVault.java:52)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1034)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
25/11/25 00:16:34 INFO SparkContext: Invoking stop() from shutdown hook
25/11/25 00:16:34 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/11/25 00:16:34 INFO SparkUI: Stopped Spark web UI at http://hadoop-master:4040
25/11/25 00:16:34 INFO YarnClientSchedulerBackend: Interrupting monitor thread
25/11/25 00:16:34 INFO YarnClientSchedulerBackend: Shutting down all executors
25/11/25 00:16:34 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/11/25 00:16:34 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
25/11/25 00:16:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/11/25 00:16:34 INFO MemoryStore: MemoryStore cleared
25/11/25 00:16:34 INFO BlockManager: BlockManager stopped
25/11/25 00:16:34 INFO BlockManagerMaster: BlockManagerMaster stopped
25/11/25 00:16:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/11/25 00:16:34 INFO SparkContext: Successfully stopped SparkContext
25/11/25 00:16:34 INFO ShutdownHookManager: Shutdown hook called
25/11/25 00:16:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c683005-6e05-49de-af15-a8ff8914a741
25/11/25 00:16:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-fea6283c-95ce-48c2-a83a-4c31f0e854bb

real	1m29.072s
user	0m26.050s
sys	0m3.891s
